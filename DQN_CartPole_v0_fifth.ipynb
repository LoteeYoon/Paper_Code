{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_CartPole-v0.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhq-9UgZo9MO"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "import gym\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import sys, os\r\n",
        "from IPython.display import clear_output\r\n",
        "from typing import List"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUZdr8GMpEr2"
      },
      "source": [
        "class Network(nn.Module):\r\n",
        "    def __init__(self, obs_dim: int, act_dim: int):\r\n",
        "        super(Network, self).__init__()\r\n",
        "\r\n",
        "        self.obs_dim = obs_dim\r\n",
        "        self.act_dim = act_dim\r\n",
        "        self.fc1 = nn.Linear(self.obs_dim, 128)\r\n",
        "        self.fc2 = nn.Linear(128, 128)\r\n",
        "        self.fc3 = nn.Linear(128, self.act_dim)\r\n",
        "\r\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\r\n",
        "        x = F.relu(self.fc1(x))\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        x = self.fc3(x)\r\n",
        "\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeKmpwLApFQO"
      },
      "source": [
        "class ReplayBuffer:\r\n",
        "    def __init__(self, memory_size: int, batch_size: int):\r\n",
        "        self.size = 0\r\n",
        "        self.tran_size = 11    ## transition s, a, r, s', d\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.max_size = memory_size\r\n",
        "        self.sample_index = []\r\n",
        "        self.train_batch = []\r\n",
        "        self.ptr = 0\r\n",
        "        \r\n",
        "        self.replay = np.zeros([self.max_size, self.tran_size], dtype=np.float32)\r\n",
        "\r\n",
        "    def store(self, \r\n",
        "              obs: np.ndarray,\r\n",
        "              action: int,\r\n",
        "              reward: float,\r\n",
        "              next_obs: np.ndarray,\r\n",
        "              done: bool,\r\n",
        "              ):\r\n",
        "        self.replay[self.ptr, :4] = obs\r\n",
        "        self.replay[self.ptr, 4:5] = action\r\n",
        "        self.replay[self.ptr, 5:6] = reward\r\n",
        "        self.replay[self.ptr, 6:10] = next_obs\r\n",
        "        self.replay[self.ptr, 10:11] = done\r\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\r\n",
        "        self.size = min(self.size + 1, self.max_size)\r\n",
        "\r\n",
        "\r\n",
        "    def sample_batch(self) -> List[int]:\r\n",
        "        sample_index = np.random.randint(low=0, high=self.size, size=self.batch_size) ##### local로 만들어줌\r\n",
        "        train_batch = np.array([self.replay[idx] for idx in sample_index]) ##### local로 만들어줌\r\n",
        "        return train_batch\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdMbRZgppHIh"
      },
      "source": [
        "class Agent:\r\n",
        "    def __init__(self, \r\n",
        "                 env: gym.Env,\r\n",
        "                 memory_size: int,\r\n",
        "                 batch_size: int,\r\n",
        "                 target_update: int,\r\n",
        "                 epsilon_decay: float,\r\n",
        "                 max_epsilon: float = 1.0,\r\n",
        "                 min_epsilon: float = 0.1,\r\n",
        "                 gamma: float = 0.99):\r\n",
        "        \r\n",
        "        self.env = env\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.memory = ReplayBuffer(memory_size, batch_size)\r\n",
        "        self.target_update = target_update\r\n",
        "        self.epsilon = max_epsilon\r\n",
        "        self.epsilon_decay = epsilon_decay\r\n",
        "        self.max_epsilon = max_epsilon\r\n",
        "        self.min_epsilon = min_epsilon\r\n",
        "        self.gamma = gamma\r\n",
        "\r\n",
        "        self.obs_dim = env.observation_space.shape[0]\r\n",
        "        self.act_dim = env.action_space.n\r\n",
        "\r\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  ####### 필요성 이해하고 넣어줌\r\n",
        "\r\n",
        "        self.dqn = Network(self.obs_dim, self.act_dim).to(self.device) # to \r\n",
        "        self.dqn_target = Network(self.obs_dim, self.act_dim).to(self.device)# to\r\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\r\n",
        "        self.dqn_target.eval()\r\n",
        "\r\n",
        "        self.optimizer = optim.Adam(self.dqn.parameters())  ####### optimizer 선언을 안 해줬었네\r\n",
        "\r\n",
        "\r\n",
        "    def update_model(self):\r\n",
        "\r\n",
        "        train_batch = self.memory.sample_batch()\r\n",
        "\r\n",
        "        loss = self.compute_loss(train_batch)\r\n",
        "\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        self.optimizer.step()\r\n",
        "\r\n",
        "        return loss.item()\r\n",
        "\r\n",
        "\r\n",
        "    def compute_loss(self, train_batch):\r\n",
        "\r\n",
        "        s = torch.FloatTensor(np.array([i[:4] for i in train_batch])).to(self.device)\r\n",
        "        a = torch.LongTensor(np.array([i[4:5] for i in train_batch])).view([-1, 1]).to(self.device)\r\n",
        "        r = torch.FloatTensor(np.array([i[5:6] for i in train_batch])).view([-1, 1]).to(self.device)\r\n",
        "        next_s = torch.FloatTensor(np.array([i[6:10] for i in train_batch])).to(self.device)\r\n",
        "        d = torch.BoolTensor(np.array([i[10] for i in train_batch])).view([-1, 1]).to(self.device)\r\n",
        "        \r\n",
        "        curr_value = self.dqn(s).gather(1, a)\r\n",
        "        next_value = self.dqn_target(next_s).max(dim=1, keepdim=True)[0].detach()\r\n",
        "\r\n",
        "        mask =  1 - d.type(torch.int32)\r\n",
        "        target = (r + self.gamma * next_value * mask).to(self.device)\r\n",
        "\r\n",
        "        loss = F.mse_loss(curr_value, target)\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "\r\n",
        "    def train(self, num_episodes: int, plot_interval: int):\r\n",
        "        \r\n",
        "        update_cnt = 0\r\n",
        "        losses = []\r\n",
        "        score = 0\r\n",
        "        scores = []\r\n",
        "\r\n",
        "        for iter in range(1, num_episodes + 1):\r\n",
        "\r\n",
        "            s = self.env.reset()\r\n",
        "            d = False\r\n",
        "            \r\n",
        "            while (not d):\r\n",
        "                #self.env.render() #######\r\n",
        "\r\n",
        "                if self.epsilon > np.random.rand(): ### rand() 로 변경 --> 0 ~ 1 사이 uniformly sample\r\n",
        "                    a = self.env.action_space.sample()\r\n",
        "                else:\r\n",
        "                    a = self.dqn.forward(torch.FloatTensor(s).to(self.device)).argmax() ####### 아래 코드랑 둘 중에 뭐가 맞는 걸까?\r\n",
        "                  # a = self.dqn(torch.FloatTensor(s).to(self.device)).argmax()\r\n",
        "                    a = a.detach().cpu().numpy()    \r\n",
        "\r\n",
        "                next_s, r, d, _ = self.env.step(a)\r\n",
        "                self.memory.store(s, a, r, next_s, d)\r\n",
        "                s = next_s\r\n",
        "                score += r\r\n",
        "                \r\n",
        "                if d:\r\n",
        "                    scores.append(score)\r\n",
        "                    score = 0\r\n",
        "\r\n",
        "                if self.memory.size >= self.batch_size:  ## len 제거함\r\n",
        "                    loss = self.update_model()\r\n",
        "                    losses.append(loss)\r\n",
        "                    update_cnt += 1\r\n",
        "\r\n",
        "                    self.epsilon = max(self.min_epsilon,\r\n",
        "                                    self.epsilon - self.epsilon_decay * (self.max_epsilon - self.min_epsilon))\r\n",
        "\r\n",
        "                    if update_cnt % self.target_update == 0:\r\n",
        "                        self.dqn_target.load_state_dict(self.dqn.state_dict())\r\n",
        "\r\n",
        "                if iter % plot_interval == 0:\r\n",
        "                    clear_output(True)\r\n",
        "                    plt.figure(figsize=(20, 5))\r\n",
        "                    plt.subplot(121)\r\n",
        "                    plt.title('iter %s. score: %s' % (iter, np.mean(scores[-10:]))) ## 가장 최근 10개 에피소드 평균 score 출력\r\n",
        "                    plt.plot(scores)\r\n",
        "                    plt.subplot(122)\r\n",
        "                    plt.title('loss')\r\n",
        "                    plt.plot(losses)\r\n",
        "                    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SX9U4vHteis"
      },
      "source": [
        "env = gym.make('CartPole-v0')\r\n",
        "memory_size = 1000\r\n",
        "batch_size = 32\r\n",
        "target_update = 100\r\n",
        "epsilon_decay = 1 / 2000\r\n",
        "\r\n",
        "agent = Agent(env, memory_size, batch_size, target_update, epsilon_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfpOnn3BtiuM"
      },
      "source": [
        "num_episodes = 100\r\n",
        "plot_interval = 200\r\n",
        "\r\n",
        "agent.train(num_episodes, plot_interval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96TT7qKP8gN3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}