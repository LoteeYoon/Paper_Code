{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "        \n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))  ## 왜 i // 2 ??\n",
    "        return position * angles\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position = tf.range(position, dtype = tf.float32)[:, tf.newaxis],  ## tf.newaxis ?\n",
    "            i = tf.range(d_model, dtype = tf.float32)[tf.newaxis, :],\n",
    "            d_model = d_model)\n",
    "        \n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        \n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        angle_rads = np.zeros(angle_rads.shape)\n",
    "        angle_rads[:, 0::2] = sines\n",
    "        angle_rads[:, 1::2] = cosines\n",
    "        pos_encoding = tf.constant(angle_rads)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        ## a = np.array([[1, 2, 3, 4, 5]], dtype = np.float32)\n",
    "        ## a.shape --> (1, 5)\n",
    "        ## a[tf.newaxis, ...].shape --> (1, 1, 5)\n",
    "        ## a[tf.newaxis, tf.newaxis, ...].shape --> (1, 1, 1, 5)\n",
    "        \n",
    "        print(pos_encoding.shape)\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    \n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \n",
    "    matmul_qk = tf.matmul(query, key, transpose_b = True)\n",
    "    \n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "    \n",
    "    # 매우 작은 음수를 어텐션 스코어 행렬에 넣어주므로 softmax 함수를 지나면 해당 위치는 0이 된다.\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "        \n",
    "    attention_weights = tf.nn.softmax(logits, axis = -1)\n",
    "    \n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, name = \"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name = name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # WQ, WK, WV 에 해당하는 dense layer 정의\n",
    "        self.query_dense = tf.keras.layers.Dense(units = d_model) ## units : Positive integer, dimensionality of the output space\n",
    "        self.key_dense = tf.keras.layers.Dense(units = d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units = d_model)\n",
    "        \n",
    "        # W0 에 해당하는 dense layer 정의\n",
    "        self.dense = tf.keras.layers.Dense(units = d_model)\n",
    "        \n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape = (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm = [0, 2, 1, 3]) # perm : index of dimension list\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 1. WQ, WK, WV 에 해당하는 dense layer 지나기\n",
    "        # q : (batch_size, query 의 문장 길이, d_model)\n",
    "        # k : (batch_size, key 의 문장 길이, d_model)\n",
    "        # v : (batch_size, value 의 문장 길이, d_model)\n",
    "        \n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "        \n",
    "        \n",
    "        # 2. head split\n",
    "        # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        # k : (batch_size, num_heads, key 의 문장 길이, d_model/num_heads)\n",
    "        # v : (batch_size, num_heads, value 의 문장 길이, d_model/num_heads)\n",
    "        \n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        \n",
    "        # 3. Scaled dot product attention\n",
    "        # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
    "        # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])\n",
    "        \n",
    "        \n",
    "        # 4. head concatenate\n",
    "        # (batch_size, query 의 문장 길이, d_model)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        \n",
    "        \n",
    "        # 5. W0 에 해당하는 dense laeyr 지나기 \n",
    "        # (batch_size, query 의 문장 길이, d_model)\n",
    "        outputs = self.dense(concat_attention)\n",
    "        \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, key 의 문장 길이)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "## 리턴된 벡터를 통해서 1의 값을 가진 위치의 열을 어텐션 스코어 행렬에서 마스킹하는 용도로 사용할 수 있다.\n",
    "## 리턴된 벡터를 scaled dot product attention 메서드에 전달하면 해당 열에 매우 작은 음수 값을 더해줘 마스킹 하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[[[0. 0. 0. 1. 1.]]]], shape=(1, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(create_padding_mask(tf.constant([[1, 21, 777, 0, 0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = tf.keras.layers.Dense(units = dff, activation = 'relu')(attention)\n",
    "# outputs = tf.keras.layers.Dense(units = d_model)(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connection & Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_layer(dff, d_model, num_heads, dropout, name = \"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape = (None, d_model), name = \"inputs\")\n",
    "    # 배치(batch)의 크기에 해당하는 첫 번째 차원 크기의 None은 크기를 여기서 정하지 않는다(어떤 배치 크기라도 가능하다)는 것을 의미\n",
    "    \n",
    "    # encoder 는 padding mask 사용\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = \"padding_mask\")\n",
    "    \n",
    "    # multi-head attention (첫 번째 서브층 / 셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name = \"attention\")({\n",
    "            'query' : inputs, 'key' : inputs, 'value' : inputs, 'mask' : padding_mask\n",
    "    })\n",
    "    \n",
    "    # dropout + residual connection + layer normalization\n",
    "    attention = tf.keras.layers.Dropout(rate = dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(inputs + attention)\n",
    "    \n",
    "    # position wise FFNN\n",
    "    outputs = tf.keras.layers.Dense(units = dff, activation = 'relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units = d_model)(outputs)\n",
    "    \n",
    "    # dropout + residual connection + layer normalization\n",
    "    outputs = tf.keras.layers.Dropout(rate = dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(attention + outputs)\n",
    "    \n",
    "    return tf.keras.Model(inputs = [inputs, padding_mask], outputs = outputs, name = name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name = \"encoder\"):\n",
    "    inputs = tf.keras.Input(shape = (None, ), name = \"inputs\")\n",
    "    \n",
    "    # encoder는 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = \"padding_mask\")\n",
    "    \n",
    "    # positioanl encoding + dropout\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))                   ## 왜 sqrt(d_model) ?\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate = dropout)(embeddings)\n",
    "    \n",
    "    # encoder를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(dff = dff, d_model = d_model, num_heads = num_heads, \n",
    "                                dropout = dropout, name = \"encoder_layer_{}\".format(i),)([outputs, padding_mask])\n",
    "        \n",
    "        \n",
    "    return tf.keras.Model(inputs = [inputs, padding_mask], outputs = outputs, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder first sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transformer는 seq2seq와 마찬가지로 Teacher Forcing을 사용하여 훈련되므로 학습 과정에서 디코더는 번역할 문장을\n",
    "## 행렬로 한 번에 입력 받는다. 여기서 트랜스포머는 현재 시점의 단어를 예측하고자 할 때, 입력 문장 행렬로부터 \n",
    "## 미래 시점의 단어까지도 참고할 수 있는 현상이 발생한다.\n",
    "## 트랜스포머는 디코더에서 현재 시점의 예측에서 미래에 있는 단어들을 참고하지 못하도록 look-ahead mask를 도입했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encoder의 sublayer인 multi head self attention과 동일한 역할을 하는데,\n",
    "## 한 가지 차이는 어텐션 스코어 행렬에서 masking을 적용한다는 것이다.\n",
    "## 자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 마스킹한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look ahead mask는 padding mask와 마찬가지로 scaled dot product attention 함수에 mask라는 인자로 전달된다.\n",
    "## padding making을 써야할 땐, padding mask를 전달, look ahead masking을 해야할 땐, look ahead mask를 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    - 인코더의 셀프 어텐션 : 패딩 마스크 전달\n",
    "    - 디코더 첫 번째 서브층 셀프 어텐션 : 룩 어헤드 마스크 전달\n",
    "    - 디코더 두 번째 서브층 인코더-디코더 어텐션 : 패딩 마스크 전달\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder의 첫 번째 sublayer에서 미래 토큰을 mask하는 함수\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    # tf.linalg.band_part(input, num_lower, num_upper) --> num_lower : # of subdiagonals to keep, if -1, keep entire lower triangle\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)  # --> upper 다 0으로\n",
    "    # padding mask도 포함 (look ahead mask를 한다고 padding mask가 필요 없는 것은 아님)\n",
    "    padding_mask = create_padding_mask(x) # --> 0인 부분 1로, 0이 아닌 부분 0으로\n",
    "    \n",
    "    return tf.maximum(look_ahead_mask, padding_mask) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder second sublayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "인코더의 첫번째 서브층 : Query = Key = Value\n",
    "디코더의 첫번째 서브층 : Query = Key = Value\n",
    "디코더의 두번째 서브층 : Query : 디코더 행렬 / Key = Value : 인코더 행렬\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 디코더의 두 번째 서브층은 멀티 헤드 어텐션을 수행한다는 점에서 이전 어텐션들과 같지만, 셀프 어텐션은 아니다.\n",
    "## 셀프 어텐션은 query, key, value가 같은 경우를 말한다.\n",
    "## 인코더-디코더 어텐션은 query가 디코더 행렬인 반면, key와 value는 인코더 행렬이기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(dff, d_model, num_heads, dropout, name = \"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape = (None, d_model), name = \"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape = (None, d_model), name = \"encoder_outputs\")\n",
    "    \n",
    "    # look ahead mask(first layer)\n",
    "    look_ahead_mask = tf.keras.Input(shape = (1, None, None), name = \"look_ahead_mask\")\n",
    "    \n",
    "    # padding mask(second layer)\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = \"padding_mask\")\n",
    "    \n",
    "    # multi-head attention (첫 번째 서브층 / masked self attention)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name = \"attention_1\")(inputs = {\n",
    "            'query' : inputs, 'key' : inputs, 'value' : inputs, # Q = K = V\n",
    "            'mask' : look_ahead_mask\n",
    "    })\n",
    "    \n",
    "    # residual connection & layer norm\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(attention1 + inputs)\n",
    "    \n",
    "    # multi-head attention (두 번째 서브층 / encoder-decoder attention)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name = \"attention_2\")(inputs = {\n",
    "            'query' : attention1, 'key' : enc_outputs, 'value' : enc_outputs, # Q != K = V\n",
    "            'mask' : padding_mask\n",
    "    })\n",
    "    \n",
    "    # dropout + resisual connection & layer norm\n",
    "    attention2 = tf.keras.layers.Dropout(rate = dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(attention2 + attention1)\n",
    "    \n",
    "    # position wise FFNN (세 번째 서브층)\n",
    "    outputs = tf.keras.layers.Dense(units = dff, activation = 'relu')(attention2)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(outputs + attention2)\n",
    "    \n",
    "    return tf.keras.Model(inputs = [inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "                         outputs = outputs,\n",
    "                         name = name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "디코더는 총 세 개의 서브층으로 구성됩니다. 첫번째와 두번째 서브층 모두 멀티 헤드 어텐션이지만, 첫번째 서브층은 mask의 인자값으로 look_ahead_mask가 들어가는 반면, 두번째 서브층은 mask의 인자값으로 padding_mask가 들어가는 것을 확인할 수 있습니다. 이는 첫번째 서브층은 마스크드 셀프 어텐션을 수행하기 때문입니다. 세 개의 서브층 모두 서브층 연산 후에는 드롭 아웃, 잔차 연결, 층 정규화가 수행되는 것을 확인할 수 있습니다.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name = 'decoder'):\n",
    "    inputs = tf.keras.Input(shape = (None, ), name = 'inputs')\n",
    "    enc_outputs = tf.keras.Input(shape = (None, d_model), name = 'encoder-outputs')\n",
    "    \n",
    "    look_ahead_mask = tf.keras.Input(shape = (1, None, None), name = 'look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = 'padding_mask')\n",
    "    \n",
    "    # positional encoding + dropout\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    outputs = tf.keras.layers.Dropout(rate = dropout)(embeddings)\n",
    "    \n",
    "    # 디코더를 num_layers개 쌓기\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(dff = dff, d_model = d_model, num_heads = num_heads,\n",
    "                               dropout = dropout, name = 'decoder_layer_{}'.format(i),\n",
    "                               )(inputs = [outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "        \n",
    "    return tf.keras.Model(inputs = [inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "                         outputs = outputs,\n",
    "                         name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, dff, d_model, num_heads, dropout, name = 'transformer'):\n",
    "    \n",
    "    # encoder의 입력\n",
    "    inputs = tf.keras.Input(shape = (None, ), name = 'inputs')\n",
    "    \n",
    "    # decoder의 입력\n",
    "    dec_inputs = tf.keras.Input(shape = (None, ), name = 'dec_inputs')\n",
    "    \n",
    "    # encoder의 padding mask\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape = (1, 1, None),\n",
    "                                             name = 'enc_padding_mask')(inputs)\n",
    "    \n",
    "    # decoder의 look ahead mask(첫 번째 서브층)\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(create_look_ahead_mask, output_shape = (1, None, None),\n",
    "                                            name = 'look_ahead_mask')(dec_inputs)\n",
    "    \n",
    "    # decoder의 padding mask(두 번째 서브층)\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape = (1, 1, None),\n",
    "                                             name = 'dec_padding_mask')(inputs)\n",
    "    \n",
    "    # encoder의 출력은 enc_outputs, decoder로 전달\n",
    "    enc_outputs = encoder(vocab_size = vocab_size, num_layers = num_layers, dff = dff, d_model = d_model,\n",
    "                         num_heads = num_heads, dropout = dropout,)(inputs = [inputs, enc_padding_mask])\n",
    "    \n",
    "    # decoder의 출력은 dec_outputs, 출력층으로 전달\n",
    "    dec_outputs = decoder(vocab_size = vocab_size, num_layers = num_layers, dff = dff, d_model = d_model,\n",
    "                         num_heads = num_heads, dropout = dropout,)(inputs = [dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "    \n",
    "    # 다음 단어 예측을 위한 출력층\n",
    "    outputs = tf.keras.layers.Dense(units = vocab_size, name = 'outputs')(dec_outputs)\n",
    "    \n",
    "    return tf.keras.Model(inputs = [inputs, dec_inputs], outputs = outputs, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer 하이퍼 파라미터 setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9000, 128)\n",
      "(1, 9000, 128)\n"
     ]
    }
   ],
   "source": [
    "small_transformer = transformer(\n",
    "    vocab_size = 9000,\n",
    "    num_layers = 4,\n",
    "    dff = 128,\n",
    "    d_model = 128,\n",
    "    num_heads = 4,\n",
    "    dropout = 0.3,\n",
    "    name = \"small_transformer\")\n",
    "\n",
    "# tf.keras.utils.plot_model(small_transformer, to_file = 'small_transformer.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실 함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape = (-1, MAX_LENGTH - 1))\n",
    "    \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits = True, reduction = 'none')(y_true, y_pred)\n",
    "    \n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    \n",
    "    loss = tf.multiply(loss, mask)\n",
    "    \n",
    "    return tf.reducde_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
